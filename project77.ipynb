{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a5ba37-984e-4425-9978-37d6c44e122d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storageAccountName = \"rout77\"\n",
    "appID = \"cb9c3ddf-79a6-44b9-8082-c69545ba5369\"\n",
    "secret = \"9Ar8Q~N67qB91fnKJY90OL6r_d-hm7EIUoex0bJD\"\n",
    "fileSystemName = \"input\"\n",
    "tenantID = \"658abc07-bff7-4d1e-beb2-32657a17c530\"\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n",
    "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n",
    "dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n",
    "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e1d449-caf1-485f-a4e8-6035fb23f2e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">neww: org.apache.spark.sql.DataFrame = [transaction_id: int, product_id: int ... 2 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">neww: org.apache.spark.sql.DataFrame = [transaction_id: int, product_id: int ... 2 more fields]\n</div>",
       "datasetInfos": [
        {
         "name": "neww",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "transaction_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "product_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "sales_amount",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {
             "__detected_date_formats": "yyyy-M-d"
            },
            "name": "sales_date",
            "nullable": true,
            "type": "date"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala \n",
    "val neww = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"abfss://input@rout77.dfs.core.windows.net/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3280040f-8e90-447f-beaf-c5a2fff5db02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure JDBC URL\n",
    "jdbcHostname = \"synapse77.sql.azuresynapse.net\"\n",
    "jdbcPort = \"1433\"\n",
    "jdbcDatabase = \"dedicatedpool77\"\n",
    "jdbcUsername = \"sqladminuser\"\n",
    "jdbcPassword = \"AnkuPanku@77\"\n",
    "\n",
    "# Connection properties\n",
    "connectionProperties = {\n",
    "    \"user\" : jdbcUsername,\n",
    "    \"password\" : jdbcPassword,\n",
    "    \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};database={jdbcDatabase};user={jdbcUsername};password={jdbcPassword};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\n",
    "\n",
    "# jdbcUrl = f\"jdbc:sqlserver://synapse77.sql.azuresynapse.net:1433;database=dedicatedpool77;user=sqladminuser;password=AnkuPanku@77;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995ecc5c-b216-42da-9049-7457fb953ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType, DateType\n",
    "\n",
    "# Read CSV files from Azure Data Lake Storage\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"abfss://input@rout77.dfs.core.windows.net/*.csv\")\n",
    "\n",
    "# Data cleaning and transformation\n",
    "df = df.withColumn(\"transaction_id\", F.col(\"transaction_id\").cast(IntegerType()))\n",
    "df = df.withColumn(\"product_id\", F.col(\"product_id\").cast(IntegerType()))\n",
    "df = df.withColumn(\"sales_amount\", F.col(\"sales_amount\").cast(DoubleType()))\n",
    "df = df.withColumn(\"sales_date\", F.to_date(F.col(\"sales_date\"), \"yyyy-MM-dd\").cast(DateType()))\n",
    "\n",
    "# Aggregate sales data\n",
    "agg_df = df.groupBy(\"product_id\").agg(F.sum(\"sales_amount\").alias(\"total_sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c561f51-97c2-427a-81a2-3259f78eb13f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write transformed data back to Azure Synapse Analytics\n",
    "agg_df.write.jdbc(url=jdbcUrl, table=\"Final\", mode=\"overwrite\", properties=connectionProperties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project77",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
